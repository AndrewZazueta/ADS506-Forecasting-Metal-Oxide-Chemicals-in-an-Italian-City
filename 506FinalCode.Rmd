---
title: "Appendix"
author: 
- "Andrew Zazueta"
- "Nava Roohi"
- "Juliet Sieland-Harris"
date: "12/3/2021"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Setting Work Directory, Loading Libraries, and Loading Data

```{r, warning=FALSE, message=FALSE}
setwd("C:/Users/mzazu/OneDrive/Documents/USD papers/506/AirQualityUCI")
library('astsa')
library('xlsx')
library('tidyverse')
library('lubridate')
library('imputeTS')
library('Rcpp')
aq <- read.xlsx('AirQualityUCI.xlsx', sheetIndex = 1, header = TRUE)
```

# Data Cleaning

### Editing the Date and Time Columns

```{r}
# The time column is reading in random dates, so we are going to fix this issue first
head(aq[2], 10)

## Splitting Time and retrieving hour
hour <- aq %>% 
  separate(Time , c("1", "2", "3", "Hour"), extra='drop') %>%
  select("Hour")

head(hour, 20)

## Adding Hour to data frame and getting rid of Time
aq$Hour <- as.numeric(unlist(hour))
aq <- aq[-2]

## Merging Date and Hour and getting rid of Date and Hour columns
aq$DateTime <- paste(aq$Date, aq$Hour)
aq <- aq[-c(1,17)]

## Making DateTime into Date and Time data type
aq$DateTime <- as.POSIXct(aq$DateTime,format="%Y-%m-%d %H", tz= "CET")
```

### Checking for missing data

```{r}
# Checking percentage of missing values in each column
cols <- colnames(aq)
for(i in 1:length(aq)){
  missing <- round(sum(is.na(aq[i]))/dim(aq)[1], 5) * 100
  print(c(cols[i], missing))
} 

# Removing NA. and NA..1
aq <- aq[-c(14, 15)]

# Finding out where the missing values are
which(is.na(aq[1]))

# Since each column is missing the same percentage of values, and aq[1]'s missing 
# values are all in an order, lets see what happens to the missing values when we 
# remove these rows.
aqNew <- aq[complete.cases(aq[1]), ]
cols <- colnames(aqNew)
for(i in 1:length(aqNew)){
  missing <- round(sum(is.na(aqNew[i]))/dim(aqNew)[1], 5) * 100
  print(c(cols[i], missing))
} 

# Now we are only missing values in DateTime
which(is.na(aqNew$DateTime))

# Since this is only two rows, it is simplest to just remove them
aqNew2 <- aqNew[complete.cases(aqNew$DateTime), ]
```

### Checking for outliers

Note: -200 was imputed for any missing values by the creators of the data set, which is hy these plots all have their lower bounds changed.

```{r}
par(mfrow = c(1,4))
cols <- colnames(aqNew2)
for(i in 1:4){
  ylim <- c(-1, max(aqNew2[i]))
  boxplot(aqNew2[i], ylab = cols[i], ylim = ylim)
}
```

NMHC is missing many values and has -200 imputed a lot, and that is the reason for the box plot not showing up. This feature will be removed later. 

```{r}
par(mfrow = c(1,4))
for(i in 5:8){
  ylim <- c(-1, max(aqNew2[i]))
  boxplot(aqNew2[i], ylab = cols[i], ylim = ylim)
}
```

```{r}
par(mfrow = c(1,4))
for(i in 9:12){
  ylim <- c(-1, max(aqNew2[i]))
  boxplot(aqNew2[i], ylab = cols[i], ylim = ylim)
}
```

```{r}
ylim <- c(0.25, max(aqNew2[13]))
boxplot(aqNew2[13], ylab = cols[13], ylim = ylim)
```

A lot of these boxplots have values of -200, which is impossible for the measurements. So, the next step is to remove any negative values from the data set. The rest of the outliers are impossible to distinguish if they are from misinputs or not, so they will be kept. 

```{r}
aqNew3 <- aqNew2
aqNew3[aqNew3 < -10] <- NA
```

We should now inspect the columns and see what percentage of values are missing from each column.

```{r}
cols <- colnames(aqNew3)
for(i in 1:length(aqNew3)){
  missing <- round(sum(is.na(aqNew3[i]))/dim(aqNew3)[1], 5) * 100
  print(c(cols[i], missing))
} 

# NMHC.GT. is missing 90% of its values, so this feature will be removed
aqNew3 <- aqNew3[-3]
```

Now, we should impute values into the missing values to avoid gaps in our time series.

```{r}
# We are using a method which finds the moving average to impute missing values
aqNew4 <- aqNew3
for(i in 1:length(aqNew4)){
  aqNew4[i] <- na_ma(aqNew4[i], k = 4, weighting = "exponential", maxgap = Inf)
}
```

```{r}
# Examining the new dimensions of our data set
dim(aqNew4)
```

### Reading cleaned data frame into new CSV file

```{r}
#write.csv(aqNew4, file = 'AirQualityCleaned.csv', row.names = FALSE)
```

# Exploritory Data Analysis

### Reading in new Data Set

```{r}
aqClean <- read_csv('AirQualityCleaned.csv')
```

Now, lets look at some of the time series plots.

### Time Series Plots

```{r}
par(mfrow = c(4,1))
tsplot(x = aqClean$DateTime, y = aqClean$CO.GT., 
       main = "True hourly averaged concentration CO in mg/m^3", ylab = "", xaxt="none", 
       cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85)
tsplot(x = aqClean$DateTime, y = aqClean$C6H6.GT., 
       main = "True hourly averaged Benzene concentration in microg/m^3", ylab = "", 
       xaxt="none", cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85)
tsplot(x = aqClean$DateTime, y = aqClean$NOx.GT., 
       main = "True hourly averaged NOx concentration in ppb", ylab = "", xaxt="none", 
       cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85)
tsplot(x = aqClean$DateTime, y = aqClean$NO2.GT., 
       main = "True hourly averaged NO2 concentration in microg/m^3", ylab = "", 
       xaxt="none", cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85)
```

After taking a look at a few time series in the data set, it is apparent that it would be better to look at daily average than to look at hourly average. Doing this would make the data more clear and less noisy, and also allow for better forecasting. 

```{r, warning=FALSE}
aqClean$Day <- format(aqClean$DateTime, format = "%Y-%m-%d")
aqDaily <- aggregate(aqClean, list(as.Date(aqClean$Day)), FUN=mean) 
aqDaily <- aqDaily[-c(14, 15)]
names(aqDaily)[1] <- "Day"
```

```{r}
# Now lets take a look at the same graphs now
par(mfrow = c(4,1))
tsplot(x = aqDaily$Day, y = aqDaily$CO.GT., 
       main = "True daily averaged concentration CO in mg/m^3", ylab = "", xlab = "Time (days)",
       cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85, xaxt="n")
tsplot(x = aqDaily$Day, y = aqDaily$C6H6.GT., 
       main = "True daily averaged Benzene concentration in microg/m^3", ylab = "", 
       cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85, xlab = "Time (days)", xaxt="none")
tsplot(x = aqDaily$Day, y = aqDaily$NOx.GT., 
       main = "True daily averaged NOx concentration in ppb", ylab = "", xlab = "Time (days)",  
       cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85, xaxt="none")
tsplot(x = aqDaily$Day, y = aqDaily$NO2.GT., 
       main = "True daily averaged NO2 concentration in microg/m^3", ylab = "", 
       xlab = "Time (days)", cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85, xaxt="none")
```

These graphs are much easier to read, so now we can move on to making the time series stationary.

### Differencing Data

```{r}
# Same graphs for reference
par(mfrow = c(4,1))
tsplot(diff(aqDaily$CO.GT.), 
       main = "True daily growth concentration of CO in mg/m^3", ylab = "", xlab = "Time (days)",
       cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85, xaxt="n")
tsplot(diff(aqDaily$C6H6.GT.), 
       main = "True daily growth of Benzene concentration in microg/m^3", ylab = "", 
       cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85, xlab = "Time (days)", xaxt="none")
tsplot(diff(aqDaily$NOx.GT.), 
       main = "True daily growth of NOx concentration in ppb", ylab = "", xlab = "Time (days)",  
       cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85, xaxt="none")
tsplot(diff(aqDaily$NO2.GT.), 
       main = "True daily growth of NO2 concentration in microg/m^3", ylab = "", 
       xlab = "Time (days)", cex.main = 0.85, cex.axis = 0.85, cex.lab = 0.85, xaxt="none")
```

The differencing appeared to make our data stationary, so now we can move on to building an ARIMA model from our data. To do this, we will need to examine the ACF and PACF for each to help determine which model to make. 

### Looking at P/ACF for Different Features

```{r}
# CO in mg/m^3
acf2(diff(aqDaily$CO.GT., 7)) # Weekly fluctuations
```

Seasonal: The PACF is cutting off a lag 1s (s = 7), whereas the ACF is tailing off at lags 1s, 7s. These results imply an SAR(1), P = 1, Q = 0.

Non-Seasonal: It appears that the PACF cuts off at lag 1, whereas the PACF tails off. This suggests an AR(1) with p = 1 and q = 0.

```{r}
# Benzene in microg/m^3 
acf2(diff(aqDaily$C6H6.GT., 7)) # Appears to have a weekly spike
```

Seasonal: The PACF is cutting off a lag 1s (s = 7), whereas the ACF is tailing off at lags 1s, 7s, and 14s. These results imply an SAR(1), P = 1, Q = 0.

Non-Seasonal: It appears that the PACF cuts off at lag 1, whereas the ACF tails off. This suggests an AR(1) with p = 1 and q = 0.

```{r}
# NOx in ppb
acf2(diff(aqDaily$NOx.GT., 7))
```

Seasonal: The PACF is cutting off a lag 1s (s = 7), whereas the ACF is tailing off at lags 1s, 7s, 14s. These results imply an SAR(1), P = 1, Q = 0.

Non-Seasonal: It appears that the PACF cuts off at lag 1, whereas the ACF tails off. This suggests an AR(1) with p = 1 and q = 0.

```{r}
# NO2 in microg/m^3
acf2((diff(aqDaily$NO2.GT., 7)))
```

Note that both NOx and NO2 are identical. The only difference between the two are the absolute values of their size. For example, NOx and NO2 will peek and dip at the same time, but their actual values differ. This is due probably to the different units of measurement. However, NOx refers to the total nitrogen oxides while NO2 is nitrogen dioxide, so there is a difference to be noted.  

# Building Models and Forecasting

### Determining the ARMA models

```{r}
# For CO
coMod <- sarima(aqDaily$CO.GT., p = 1, d = 1, q = 1, P = 1, D = 0, Q = 1, S = 7)
```

It appears to be decent. 1 of our Q-statistic p-values are less than 0.05, but the majority are not. It is important to have values greater than 0.05, because then we do not need to reject the null hypothesis that the noise is white. Adding q = 1 and Q = 1 helps with model evaluation metrics. 

```{r, warning=FALSE}
# For Benzene
benzeneMod <- sarima(aqDaily$C6H6.GT., p = 1, d = 1, q = 1, P = 1, D = 0, Q = 1, S = 7)
```

Adding q = 1 and q = 1 to our model improves the Q-statistic p-values.

```{r, warning=FALSE}
# For NOx
noxMod <- sarima(aqDaily$NOx.GT., p = 1, d = 1, q = 1, P = 2, D = 0, Q = 1, S = 7)
```

Adding q = 1, Q = 1, and P = 2 helps with our model. Since the first half of the data is different from the second half, we are getting bad results from the different statistical measures that cannot be changed. If P = 1, it does not allow for Q = 1, so this is why P = 2 instead. 

```{r}
# For NO2
no2Mod <- sarima(aqDaily$NO2.GT., p = 1, d = 1, q = 1, P = 2, D = 0, Q = 1, S = 7)
```

This will share the same parameters as NOx for the seasonal ARMA model. 

### Forecasting 

The 'Future' portion of our graph is where the forecasting is made. The black line represents the actual data collected while the red line shows our ARMA model prediction. The grayed out areas are confidence intervals. 

```{r}
# Plotting the final month of time series for CO
x <- ts(aqDaily$CO.GT., start = decimal_date(as.Date("2004-03-10")), frequency = 365)
CO <- window(x, start=decimal_date(as.Date("2004-03-10")),
            end=decimal_date(as.Date("2005-03-04")))
sarima.for(CO, n.ahead = 32, p = 1, d = 1, q = 1, P = 1, D = 0, Q = 1, S = 7, plot.all=FALSE,
           main = "Forecasting CO")
text(decimal_date(as.Date("2005-02-04")), 5, "PAST")
text(decimal_date(as.Date("2005-03-20")), 5, "FUTURE")
abline(v=decimal_date(as.Date("2005-03-04")), lty=2, col=4)
lines(x)
```

```{r}
# Plotting the final month of time series for Benzene
x <- ts(aqDaily$C6H6.GT., start = decimal_date(as.Date("2004-03-10")), frequency = 365)
C6H6 <- window(x, start=decimal_date(as.Date("2004-03-10")),
            end=decimal_date(as.Date("2005-03-04")))
sarima.for(C6H6, n.ahead = 32, p = 1, d = 1, q = 1, P = 1, D = 0, Q = 1, S = 7,
           plot.all=FALSE, main = "Forecasting Benzene")
text(decimal_date(as.Date("2005-02-04")), 22, "PAST")
text(decimal_date(as.Date("2005-03-20")), 22, "FUTURE")
abline(v=decimal_date(as.Date("2005-03-04")), lty=2, col=4)
lines(x)
```

```{r}
# Plotting the final month of time series for NOx
x <- ts(aqDaily$NOx.GT., start = decimal_date(as.Date("2004-03-10")), frequency = 365)
NOx<- window(x, start=decimal_date(as.Date("2004-03-10")),
            end=decimal_date(as.Date("2005-03-04")))
sarima.for(NOx, n.ahead = 32, p = 1, d = 1, q = 1, P = 2, D = 0, Q = 1, S = 7,
           plot.all=FALSE, main = "Forecasting Total Nitrogen Oxides")
text(decimal_date(as.Date("2005-02-04")), 700, "PAST")
text(decimal_date(as.Date("2005-03-20")), 700, "FUTURE")
abline(v=decimal_date(as.Date("2005-03-04")), lty=2, col=4)
lines(x)
```

```{r}
# Plotting the final month of time series for NO2
x <- ts(aqDaily$NO2.GT., start = decimal_date(as.Date("2004-03-10")), frequency = 365)
NO2 <- window(x, start=decimal_date(as.Date("2004-03-10")),
              end=decimal_date(as.Date("2005-03-04")))
sarima.for(NO2, n.ahead = 32, p = 1, d = 1, q = 1, P = 2, D = 0, Q = 1, S = 7,
           plot.all=FALSE, main = "Forecasting Nitrogen Dioxide")
text(decimal_date(as.Date("2005-02-12")), 85, "PAST")
text(decimal_date(as.Date("2005-03-20")), 85, "FUTURE")
abline(v=decimal_date(as.Date("2005-03-04")), lty=2, col=4)
lines(x)
```

This model fits the least well at the end. This is because it takes a sudden dove down which is not predictable by seeing how the time series behaves before.

# Model Evaluation

### Table of AIC, AICc, and BIC for each model

```{r}
# Inputting the metrics previously saved 
metrics <- c(coMod$AIC, coMod$AICc, coMod$BIC, benzeneMod$AIC, benzeneMod$AICc, 
             benzeneMod$BIC, noxMod$AIC, noxMod$AICc, noxMod$BIC, no2Mod$AIC, 
             no2Mod$AICc, no2Mod$BIC)
metrics <- round(metrics, 2)
tab <- matrix(metrics, ncol=3, byrow=TRUE)
colnames(tab) <- c('AIC','AICc','BIC')
rownames(tab) <- c('CO','Benzene', 'NOx', 'NO2')
tab <- as.table(tab)
tab
```

From this table, we can see that our SARMA model best fit for CO. In the end, the parameters were the same for CO and Benzene, while NOx and NO2 were identical. This can infer that CO and Benzene are similar in how they come to be in the air, but statements like this are best left to climate scientists.  